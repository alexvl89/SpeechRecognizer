from pydub import effects
import whisperx
import gc
import torch
import os
from pathlib import Path
# from main import start_bot
from pydub import AudioSegment
from transformers import pipeline


AUDIO_SAVE_NORM = "audio_files\\normalized"


class SpeechRecognizer:


    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "int8"  # –∏–ª–∏ "float16"/"float32"
    batch_size = 5
    hf_token = os.getenv('YOUR_HF_TOKEN')

    @staticmethod
    def log_devices():
        if torch.cuda.is_available():
            print("CUDA is available!")
            for i in range(torch.cuda.device_count()):
                print(f"Device {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("CUDA is NOT available. Using CPU.")

        if hasattr(torch, "xpu") and torch.xpu.is_available():
            print("XPU is available")
        else:
            print("XPU is not available")

    @staticmethod
    def preprocess_audio(input_path: str, output_path: str) -> str:
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
        print("–§–∞–π–ª –Ω–∞–π–¥–µ–Ω:", input_path)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ WAV 16bit mono 16kHz PCM + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        audio = AudioSegment.from_file(input_path, format="ogg")
        audio = audio.set_channels(1).set_frame_rate(16000).set_sample_width(2)
        audio = effects.normalize(audio)

        # –î–æ–±–∞–≤–∏–º —Ç–∏—à–∏–Ω—É –Ω–∞ —Å–ª—É—á–∞–π –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞—É–¥–∏–æ
        silence = AudioSegment.silent(duration=5000)
        audio += silence

        audio.export(output_path, format="wav")
        print(f"Audio preprocessed and saved to: {output_path}")

        # –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        try:
            os.remove(input_path)
            print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª—ë–Ω: {input_path}")
        except OSError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ {input_path}: {e}")

        return output_path

    @classmethod
    def transcribe_audio(cls, input_ogg_path: str) -> str:
        cls.log_devices()
        wav_path = os.path.join(
            AUDIO_SAVE_NORM, os.path.basename(input_ogg_path))
        # –≤–µ—Ä–Ω—É–ª–∏ —Ñ–∞–π–ª
        cls.preprocess_audio(input_ogg_path, wav_path)

        # 1. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
        model = whisperx.load_model(
            "large-v2", cls.device, compute_type=cls.compute_type)
        audio_tensor = whisperx.load_audio(wav_path)
        result = model.transcribe(
            audio_tensor, batch_size=cls.batch_size, language='ru')
        print("–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã (–¥–æ alignment):")
        print(result["segments"])

        # # 2. –ê–ª–∏–≥–Ω–º–µ–Ω—Ç
        # model_a, metadata = whisperx.load_align_model(
        #     language_code=result["language"],
        #     device=cls.device
        # )
        # result = whisperx.align(
        #     result["segments"], model_a, metadata, wav_path,
        #     device=cls.device,
        #     return_char_alignments=False
        # )
        # print("–°–µ–≥–º–µ–Ω—Ç—ã –ø–æ—Å–ª–µ alignment:")
        # print(result["segments"])

        # # 3. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        # diarize_model = whisperx.diarize.DiarizationPipeline(
        #     use_auth_token=cls.hf_token,
        #     device=cls.device
        # )
        # diarize_segments = diarize_model(wav_path)
        # result = whisperx.assign_word_speakers(diarize_segments, result)

        # print("–°–µ–≥–º–µ–Ω—Ç—ã —Å —É–∫–∞–∑–∞–Ω–∏–µ–º speaker ID:")
        # print(result["segments"])

        # –£–¥–∞–ª–µ–Ω–∏–µ WAV-—Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        try:
            os.remove(wav_path)
            print(f"WAV-—Ñ–∞–π–ª —É–¥–∞–ª—ë–Ω: {wav_path}")
        except OSError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ WAV-—Ñ–∞–π–ª–∞ {wav_path}: {e}")

        # –û—á–∏—Å—Ç–∫–∞
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # –°–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        text = " ".join([seg["text"] for seg in result["segments"]])
        return text.strip()

# # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
# YOUR_HF_TOKEN = os.getenv('YOUR_HF_TOKEN')

# if torch.cuda.is_available():
#     print("CUDA is available!")
#     print(f"Device count: {torch.cuda.device_count()}")
#     for i in range(torch.cuda.device_count()):
#         print(f"Device {i}: {torch.cuda.get_device_name(i)}")
# else:
#     print("CUDA is NOT available. Using CPU.")


# if torch.xpu.is_available():
#     print("xpu is available")
# else:
#     print("xpu is not available")

# device = "cpu"
# audio_file = "audio_2025-07-11_14-50-05.ogg"
# input_ogg = "audio_files/854924596_25.ogg"
# batch_size = 16 # reduce if low on GPU mem
# compute_type = "int8" # change to "int8" if low on GPU mem (may reduce accuracy)

# # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞
# if not os.path.exists(input_ogg):
#     raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_ogg}")
# print("–§–∞–π–ª –Ω–∞–π–¥–µ–Ω:", input_ogg)


# # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ogg ‚Üí wav (–º–æ–Ω–æ, 16–∫–ì—Ü, 16bit PCM)
# audio = AudioSegment.from_file(input_ogg, format="ogg")
# audio = audio.set_channels(1).set_frame_rate(16000).set_sample_width(2)
# # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ
# audio = effects.normalize(audio)


# silence = AudioSegment.silent(duration=5000)  # 1 —Å–µ–∫
# audio = audio + silence

# print(f"Duration (ms): {len(audio)}")  # –º–∏–Ω–∏–º—É–º –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ > 1000

# input_ogg = "silence.wav"
# # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ WAV
# audio.export(input_ogg, format="wav")


# # # –ú–æ–∂–Ω–æ "base" –∏–ª–∏ "tiny"
# # model = whisperx.load_model("small", device, compute_type=compute_type)
# # # model = whisperx.load_model("large-v2", device, compute_type=compute_type)
# # result = model.transcribe(input_ogg, batch_size=5,
# #                           language='ru')
# # # result = model.transcribe(input_ogg, language='ru')
# # print(result)

# # normalized_wav = "normalized_audio.wav"
# # audio.export(normalized_wav, format="wav")

# # 1. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
# model = whisperx.load_model("large-v2", device, compute_type=compute_type)
# audio_tensor = whisperx.load_audio(input_ogg)
# result = model.transcribe(audio_tensor, batch_size=batch_size, language='ru')
# print(result["segments"])  # before alignment

# # 2. –ê–ª–∏–≥–Ω–º–µ–Ω—Ç
# model_a, metadata = whisperx.load_align_model(
#     language_code=result["language"], device=device)
# result = whisperx.align(
#     result["segments"],
#     model_a,
#     metadata,
#     input_ogg,  # –ø–µ—Ä–µ–¥–∞—ë–º –ø—É—Ç—å, –∞ –Ω–µ –º–∞—Å—Å–∏–≤
#     device,
#     return_char_alignments=False
# )
# print(result["segments"])  # after alignment

# # 3. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
# diarize_model = whisperx.diarize.DiarizationPipeline(
#     use_auth_token=YOUR_HF_TOKEN,
#     device=device
# )

# diarize_segments = diarize_model(input_ogg)  # –ø—É—Ç—å –∫ wav-—Ñ–∞–π–ª—É
# result = whisperx.assign_word_speakers(diarize_segments, result)

# print(diarize_segments)
# print(result["segments"])  # —Å —É–∫–∞–∑–∞–Ω–∏–µ–º speaker ID

# # –û—á–∏—Å—Ç–∫–∞ GPU (–µ—Å–ª–∏ –Ω–∞–¥–æ)
# gc.collect()
# if torch.cuda.is_available():
#     torch.cuda.empty_cache()

# # # –ó–∞–ø—É—Å–∫ Telegram-–±–æ—Ç–∞
# # if __name__ == "__main__":
# #     start_bot()

# print("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ü–∏–∫–ª–∞")


    @classmethod
    def summarize_text(cls, text: str) -> str:
        print("\nüìå –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ –ø–µ—Ä–µ—Å–∫–∞–∑–∞...")
        summarizer = pipeline(
            "summarization", model="cointegrated/rut5-base-summarizer")
        # summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6",
        #                       device=0 if cls.device == "xpu" else -1)
        summary = summarizer(text, max_length=60,
                             min_length=10, do_sample=False)
        print(summary)
        return summary[0]["summary_text"].strip()


if __name__ == "__main__":
    recognizer = SpeechRecognizer()

    # wav_path = os.path.join(
    #     "audio_files\input", os.path.basename("854924596_111.ogg"))
    # transcript = recognizer.transcribe_audio(wav_path)

    text = "–ù–∞ —Ä—ã–±–∞–ª–∫—É, –°–∞–Ω—è, –Ω–∞ —Ä—ã–±–∞–ª–∫—É."
    transcript = text
    print("\nüì¢ –ü–µ—Ä–µ—Å–∫–∞–∑:")
    summary = recognizer.summarize_text(transcript)
    print(summary)
