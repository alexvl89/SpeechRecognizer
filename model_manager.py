import gc
import os
import logging
from pathlib import Path
from typing import Optional
from faster_whisper import WhisperModel
import torch
import uuid


logger = logging.getLogger(__name__)


class WhisperModelManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ faster-whisper."""

    _instance: Optional["WhisperModelManager"] = None
    _model: Optional[WhisperModel] = None

    def __init__(
        self,
        device: str = "cuda",
        compute_type: str = "float16",
        model_name: str = "Systran/faster-whisper-large-v2",
        download_root: Optional[str] = None


    ):
        self.device = device
        self.compute_type = compute_type
        self.model_name = model_name
        self.download_root = download_root or os.path.join(
            os.getcwd(), "app", "models", "faster-whisper-large-v2")
        self.required_files = ["model.bin", "tokenizer.json",
                               "config.json"]

        self.pid = os.getpid()
        self.uid = str(uuid.uuid4())[:8]
        logger.info(
            f"[PID {self.pid}] WhisperModelManager —Å–æ–∑–¥–∞–Ω, uid={self.uid}")

    def __new__(cls, *args, **kwargs):
        """–°–∏–Ω–≥–ª—Ç–æ–Ω: —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_model(self) -> WhisperModel:
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏."""
        if self._model is not None:
            return self._model

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ faster-whisper –Ω–∞ {self.device}...")
        self._model = self._load_model()
        logger.info(
            f"[PID {os.getpid()}] get_model ‚Üí _model={'exists' if self._model else 'None'}")
        return self._model

    def _load_model(self) -> WhisperModel:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ ‚Üí –∏–Ω–∞—á–µ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Å HF."""
        model_dir = Path(self.download_root)

        # 1. –ü—Ä—è–º–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
        if self._is_valid_model_dir(model_dir):
            return self._load_from_path(model_dir)

        # 2. –ú–æ–¥–µ–ª—å –≤ –∫—ç—à–µ Hugging Face
        hf_cache_dir = model_dir / "models--Systran--faster-whisper-large-v2"
        snapshot_path = self._find_latest_snapshot(hf_cache_dir)
        if snapshot_path and self._is_valid_model_dir(snapshot_path):
            return self._load_from_path(snapshot_path)

        # 3. –ó–∞–≥—Ä—É–∑–∫–∞ —Å HF
        logger.info("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∫–∞ —Å Hugging Face...")
        model_dir.mkdir(parents=True, exist_ok=True)

        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            logger.warning(
                "HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ–π –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞.")

        try:
            model = WhisperModel(
                model_size_or_path=self.model_name,
                device=self.device,
                compute_type=self.compute_type,
                download_root=str(model_dir),
                local_files_only=False,
            )
            logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å Hugging Face.")
            return model
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å Hugging Face: {e}")
            raise RuntimeError(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å faster-whisper.") from e

    def _is_valid_model_dir(self, path: Path) -> bool:
        if not path.exists() or not path.is_dir():
            return False
        missing = [f for f in self.required_files if not (path / f).is_file()]
        if missing:
            logger.info(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –≤ {path}: {missing}")
            return False
        logger.info(f"–í–∞–ª–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {path}")
        return True

    def _find_latest_snapshot(self, hf_cache_dir: Path) -> Optional[Path]:
        snapshots_dir = hf_cache_dir / "snapshots"
        if not snapshots_dir.exists():
            return None

        hashes = [d for d in snapshots_dir.iterdir() if d.is_dir()]
        if not hashes:
            return None

        latest = max(hashes, key=lambda x: x.name)
        candidate = latest
        if self._is_valid_model_dir(candidate):
            return candidate
        logger.warning(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π snapshot –ø–æ–≤—Ä–µ–∂–¥—ë–Ω: {candidate}")
        return None

    def _load_from_path(self, path: Path) -> WhisperModel:
        try:
            model = WhisperModel(
                model_size_or_path=str(path),
                device=self.device,
                compute_type=self.compute_type,
                local_files_only=True
            )
            logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ –∏–∑: {path}")
            return model
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ {path}: {e}")
            raise


    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ –º–æ–¥–µ–ª–∏ –∏ GPU (–ø–æ –∂–µ–ª–∞–Ω–∏—é)."""
        logger.info(f"Before cleanup: object={self._model}")

        if self._model is not None:
            del self._model
            self._model = None
            gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # üî• –ö–õ–Æ–ß–ï–í–ê–Ø –§–ò–®–ö–ê ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–¥–∞—Ç—å –ø–∞–º—è—Ç—å –û–°
        try:
            import ctypes
            libc = ctypes.CDLL("libc.so.6")
            libc.malloc_trim(0)
            logger.info("malloc_trim(0) –≤—ã–ø–æ–ª–Ω–µ–Ω ‚Äî –ø–∞–º—è—Ç—å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∞ –û–°.")
        except Exception as e:
            logger.warning(f"malloc_trim –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        logger.info("–ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏.")
        logger.info(f"After cleanup: object={self._model}")

